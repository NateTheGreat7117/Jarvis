{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2df749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0be85a4",
   "metadata": {},
   "source": [
    "# Get DialoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5eaf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094082b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca5186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello there\"\n",
    "tokenized = tokenizer.encode_plus(text + tokenizer.eos_token,\n",
    "                                  return_tensors=\"pt\")['input_ids']\n",
    "logits = model(tokenized.to(\"cuda\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5771786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e79e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(logits, 2)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 10\n",
    "for i in range(max_len):\n",
    "    out = model(torch.tensor(tokenized).to(\"cuda\"))\n",
    "    new = torch.argmax(out[0], 2)[0][-1]\n",
    "    tokenized = torch.unsqueeze(torch.concat((tokenized[0], torch.tensor([new]))), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24717aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b2ca01",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a802abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs():\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    conversations = [\"\"]\n",
    "    lines = \"\"\n",
    "    counter = 0\n",
    "    general = False\n",
    "\n",
    "    with open(\"/media/nathanmon/389E28739E282BB6/Users/Natha/Datasets/MyJarvisConversation/conversation.txt\", \"r\") as f:\n",
    "        for line in f.readlines()[1:]:\n",
    "            if line[0] == \"G\":\n",
    "                conversations[counter] = lines[:-1].lower().replace(\"\\n\", \" \")\n",
    "                lines = \"\"\n",
    "                conversations.append(\"\")\n",
    "                counter += 1\n",
    "                \n",
    "                general = True\n",
    "            if line[0] == \"C\":\n",
    "                conversations[counter] = lines[:-1].lower().replace(\"\\n\", \" \")\n",
    "                lines = \"\"\n",
    "                conversations.append(\"\")\n",
    "                counter += 1\n",
    "            if line[0] == \"U\":\n",
    "                lines += line\n",
    "            elif line[0] == \"J\":\n",
    "                line = line.replace(\"/u\", \"/u \") + \"/t \"\n",
    "                lines += line\n",
    "                if general:\n",
    "                    conversations[counter] = lines[:-1].lower().replace(\"\\n\", \" \")\n",
    "                    lines = \"\"\n",
    "                    conversations.append(\"\")\n",
    "                    counter += 1\n",
    "\n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcadccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = readLangs()[:-1]\n",
    "np.random.shuffle(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081718a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff846f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"sir /t user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6393f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd07910",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "\n",
    "    loss_object = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    pred_flat = pred.view(-1, pred.size(-1))\n",
    "    label_flat = label.view(-1)\n",
    "    \n",
    "    pred_masked = pred_flat[mask.view(-1)]\n",
    "    label_masked = label_flat[mask.view(-1)]\n",
    "    \n",
    "    loss = loss_object(pred_masked, label_masked)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = torch.argmax(pred, axis=2)\n",
    "    label = label.to(pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = match.to(torch.float32)\n",
    "    mask = mask.to(torch.float32)\n",
    "    return torch.sum(match)/torch.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8256e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"/u shopping\", \"/u todolist\", \"/u wiki\", \"/u volume\", \"/a/\"]\n",
    "filenames = [\"shopping_items\", \"todo_list_items\", \"wiki_queries\", \"volumes\", \"apps\"]\n",
    "augments = {\"shopping_items\": [], \"todo_list_items\": [],\n",
    "            \"wiki_queries\": [], \"volumes\": [], \"apps\": []}\n",
    "\n",
    "for keyword, filename in zip(keywords, filenames):\n",
    "    with open(f\"/media/nathanmon/389E28739E282BB6/Users/Natha/Datasets/MyJarvisConversation/{filename}.txt\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            augments[filename].append(line.replace(\"\\n\", \"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33305e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [\"zero\", \"one\", \"two\", \"three\", \"four\", \n",
    "           \"five\", \"six\", \"seven\", \"eight\", \"nine\",\n",
    "           \"ten\", \"eleven\", \"twelve\", \"thirteen\",\n",
    "           \"fourteen\", \"fifteen\", \"sixteen\", \n",
    "           \"seventeen\", \"eighteen\", \"nineteen\",\n",
    "           \"twenty\", \"thirty\", \"forty\", \"fifty\",\n",
    "           \"sixty\", \"seventy\", \"eighty\", \"ninety\",\n",
    "           \"hundred\", \"thousand\", \"million\", \"billion\",\n",
    "           \"trillion\", \"quadrillion\", \"quintillion\", \"mute\", \"?\"]\n",
    "\n",
    "def sentence2num(sentence):\n",
    "    words = sentence.split(\" \")\n",
    "    filtered = []\n",
    "    for word in words:\n",
    "        if word.lower() in numbers:\n",
    "            filtered.append(word)\n",
    "\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "def find_tgt(response, loc=\"'\"):\n",
    "    try:\n",
    "        lower = response.index(loc) + len(loc)\n",
    "        upper = response[lower:].index(loc) + lower\n",
    "        return response[lower:upper]\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "def augment(inp, tgt):\n",
    "    keywords = [\"/u shopping\", \"/u todolist\", \"/u wiki\", \"/u volume\", \"/a\"]\n",
    "    filenames = [\"shopping_items\", \"todo_list_items\", \"wiki_queries\", \"volumes\", \"apps\"]\n",
    "\n",
    "    for keyword, filename in zip(keywords, filenames):\n",
    "        if keyword in tgt or keyword in inp:\n",
    "            if keyword == \"/u volume\":\n",
    "                prev_item = sentence2num(find_tgt(tgt))\n",
    "            elif keyword == \"/a\":\n",
    "                prev_item = find_tgt(inp, \"/a\")[1:-1]\n",
    "            else:\n",
    "                prev_item = find_tgt(tgt)\n",
    "\n",
    "            if keyword != \"/uvolume\" or (prev_item != \"?\"and prev_item.lower() != \"mute\"):\n",
    "                replacement = random.choice(augments[filename])\n",
    "                inp = inp.replace(prev_item, replacement)\n",
    "                if keyword == \"/a\":\n",
    "                    prev_item = find_tgt(tgt)[1:-1]\n",
    "\n",
    "                if prev_item is not None:\n",
    "                    tgt = tgt.replace(prev_item, replacement)\n",
    "                \n",
    "    return inp.replace(\"/a\", \"\"), tgt.replace(\"/a\", \"\")\n",
    "\n",
    "def split_lines(line, sep):\n",
    "    if line.find(sep) != -1:\n",
    "        lines = []\n",
    "        index = 0\n",
    "        for i in range(line.count(sep)):\n",
    "            lines.append(line[index:line.index(sep, index)+len(sep)])\n",
    "            index = line.index(sep, index)+len(sep)+1\n",
    "        return lines\n",
    "    return [line]\n",
    "\n",
    "def augment_tokens(tokenizer, tokens):\n",
    "    string = tokenizer.decode(tokens)\n",
    "    \n",
    "    lines = split_lines(string, \"/t\")\n",
    "    for i in range(len(lines)):\n",
    "        if \"\\n\" in lines[i]:\n",
    "            split = lines[i].split(\"\\n\", 1)\n",
    "            inp, tgt = augment(split[0], split[1])\n",
    "            lines[i] = inp + \"\\n\" + tgt\n",
    "        lines[i] = lines[i]\n",
    "    augmented = ''.join(lines)\n",
    "    \n",
    "    tokens = tokenizer.encode(augmented)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cc4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(conversations, model, optimizer, criterion, \n",
    "                window_length, scheduler, print_every, plot_every, train=True):\n",
    "    global batch_size\n",
    "    global tokenizer\n",
    "    \n",
    "    total_loss = 0\n",
    "    plot_total_loss = 0  # Reset every plot_every\n",
    "    plot_losses = []\n",
    "    \n",
    "    plot_learning_rates = []\n",
    "    \n",
    "    counter = 1\n",
    "    num_tokens = 0\n",
    "    \n",
    "    for conversation in conversations:\n",
    "        tokenized = tokenizer.encode(conversation + tokenizer.eos_token)\n",
    "        print(\"Conversation \", counter)\n",
    "        for i in range(2, len(tokenized)):\n",
    "            segment = tokenized[max(i-window_length, 0):i]\n",
    "            augmented = augment_tokens(tokenizer, segment)\n",
    "            inp = augmented[:-1]\n",
    "            tgt = augmented[-1]\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(torch.tensor([inp]).to(\"cuda\"))[0][0][-1]\n",
    "\n",
    "            loss = criterion(logits, torch.tensor(tgt).to(\"cuda\"))\n",
    "\n",
    "            if train:\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                \n",
    "            total_loss += loss.item()\n",
    "            num_tokens += 1\n",
    "                \n",
    "        ######## metrics ########\n",
    "        counter += 1\n",
    "        if not train:\n",
    "            scheduler.step(total_loss / num_tokens)\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "                print_loss_avg = (total_loss / num_tokens) / print_every\n",
    "                total_loss = 0\n",
    "                print('Conversation  %d: %d%% %.4f %.7f' % (counter, \n",
    "                      counter / len(conversations) * 100, \n",
    "                      print_loss_avg, optimizer.param_groups[0][\"lr\"]))\n",
    "\n",
    "        if counter % plot_every == 0:\n",
    "            plot_loss_avg = (plot_total_loss / num_tokens) / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "\n",
    "            plot_total_loss = 0\n",
    "            plot_learning_rates.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return plot_losses, plot_learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb9672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c9c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's \n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, best_valid_loss=float('inf')\n",
    "    ):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        \n",
    "    def __call__(\n",
    "        self, current_valid_loss, \n",
    "        epoch, model, optimizer, criterion\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            print(f\"Best validation loss: {self.best_valid_loss}\")\n",
    "            print(f\"Saving best model for epoch: {epoch+1}\")\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,\n",
    "                }, 'checkpoints/best_causal_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38685547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(train_dataloader, val_dataloader, transformer, n_epochs, learning_rate=0.001,\n",
    "def train(train_conversations, val_conversations, model, n_epochs,\n",
    "          window_length=25, learning_rate=1e-3, print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    global d_model\n",
    "    plot_train_losses = []\n",
    "    plot_val_losses = []\n",
    "    \n",
    "    plot_learning_rates = []\n",
    "\n",
    "    save_best = SaveBestModel(best_valid_loss=.59)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                           betas=(0.95, 0.9995), eps=1e-9)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", factor=0.05, patience=300)        \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        plot_train_loss, _ = train_epoch(train_conversations, model, optimizer, criterion, \n",
    "                                                          window_length, scheduler, print_every, plot_every)\n",
    "        \n",
    "        plot_train_losses = np.concatenate((plot_train_losses, plot_train_loss))\n",
    "        \n",
    "        # Evaluate validation dataloader\n",
    "        plot_val_loss, plot_learning_rate = train_epoch(val_conversations, model, optimizer, criterion, \n",
    "                                       window_length, scheduler, print_every, plot_every, train=False)\n",
    "        plot_val_losses = np.concatenate((plot_val_losses, plot_val_loss))\n",
    "        plot_learning_rates = np.concatenate((plot_learning_rates, plot_learning_rate))\n",
    "        \n",
    "        print('%s (%d %d%%) %.4f %.4' % (timeSince(start, epoch / n_epochs),\n",
    "                epoch, epoch / n_epochs * 100))\n",
    "        \n",
    "        save_best(val_loss, epoch, model, optimizer, criterion)\n",
    "\n",
    "    showPlot(plot_train_losses, \"loss\", plot_val_losses, \"val_loss\")\n",
    "    showPlot(plot_learning_rates, \"learning rate\")\n",
    "    return plot_train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2006fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points, points_name, points2=None, points2_name=None):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.5)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    if points2 != None:\n",
    "        plt.plot(np.arange(len(points)), points, points2)\n",
    "        plt.legend([points_name, points2_name])\n",
    "    else:\n",
    "        plt.plot(points)\n",
    "        plt.legend([points_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a53fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057f3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(.9 * len(conversations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f318a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0f5570",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_conversations = conversations[:int(.9 * len(conversations))]\n",
    "val_conversations = conversations[int(.9 * len(conversations)):]\n",
    "\n",
    "history = train(train_conversations, val_conversations, model, 2, \n",
    "                window_length=65, learning_rate=1e-5, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa4e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello there\"\n",
    "tokenized = tokenizer.encode_plus(text + tokenizer.eos_token,\n",
    "                                  return_tensors=\"pt\")['input_ids']\n",
    "logits = model(tokenized)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 10\n",
    "for i in range(max_len):\n",
    "    out = model(torch.tensor(tokenized))\n",
    "    new = torch.argmax(out[0], 2)[0][-1]\n",
    "    tokenized = torch.unsqueeze(torch.concat((tokenized[0], torch.tensor([new]))), 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
