{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d87761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from pytorch_model_summary import summary\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import random\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f4080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import requests\n",
    "# import beautifulsoup4 as bs4\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a464cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_encoder = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d1407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 101\n",
    "EOS_token = 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z'/!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"'\", \" '\", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1589034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs():\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    lines = []\n",
    "    counter = 0\n",
    "\n",
    "    with open(\"/media/nathanmon/389E28739E282BB6/Users/Natha/Datasets/MyJarvisConversation/conversation.txt\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            if line[0] == \"U\":\n",
    "                lines.append(\"\")\n",
    "                lines[counter] += line[6:] + \"/t\"\n",
    "            elif line[0] == \"J\":\n",
    "                line = line.replace(\"/u\", \"/u \")\n",
    "                lines[counter] += line[8:]\n",
    "                counter += 1\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4d3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4cefc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData():\n",
    "    pairs = readLangs()\n",
    "    pairs = filterPairs(pairs)\n",
    "    for i, pair in enumerate(pairs):\n",
    "        pairs[i] = pair.lower().replace(\"\\n\", \"\").split(\"/t\")\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109d538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"/u shopping\", \"/u todolist\", \"/u wiki\", \"/u volume\", \"/a/\"]\n",
    "filenames = [\"shopping_items\", \"todo_list_items\", \"wiki_queries\", \"volumes\", \"apps\"]\n",
    "augments = {\"shopping_items\": [], \"todo_list_items\": [],\n",
    "            \"wiki_queries\": [], \"volumes\": [], \"apps\": []}\n",
    "\n",
    "for keyword, filename in zip(keywords, filenames):\n",
    "    with open(f\"/media/nathanmon/389E28739E282BB6/Users/Natha/Datasets/MyJarvisConversation/{filename}.txt\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            augments[filename].append(line.replace(\"\\n\", \"\").strip())\n",
    "            \n",
    "numbers = [\"zero\", \"one\", \"two\", \"three\", \"four\", \n",
    "   \"five\", \"six\", \"seven\", \"eight\", \"nine\",\n",
    "   \"ten\", \"eleven\", \"twelve\", \"thirteen\",\n",
    "   \"fourteen\", \"fifteen\", \"sixteen\", \n",
    "   \"seventeen\", \"eighteen\", \"nineteen\",\n",
    "   \"twenty\", \"thirty\", \"forty\", \"fifty\",\n",
    "   \"sixty\", \"seventy\", \"eighty\", \"ninety\",\n",
    "   \"hundred\", \"thousand\", \"million\", \"billion\",\n",
    "   \"trillion\", \"quadrillion\", \"quintillion\", \"mute\", \"?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f05c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sentence2num(sentence):\n",
    "#     words = sentence.split(\" \")\n",
    "#     filtered = []\n",
    "#     for word in words:\n",
    "#         if word.lower() in numbers:\n",
    "#             filtered.append(word)\n",
    "\n",
    "#     return \" \".join(filtered)\n",
    "\n",
    "# def find_tgt(response, loc=\"'\"):\n",
    "#     lower = response.index(loc) + len(loc)\n",
    "#     upper = response[lower:].index(loc) + lower\n",
    "#     return response[lower:upper]\n",
    "\n",
    "# def augment(inp, tgt):\n",
    "#     keywords = [\"/u shopping\", \"/u todolist\", \"/u wiki\", \"/u volume\", \"/a\"]\n",
    "#     filenames = [\"shopping_items\", \"todo_list_items\", \"wiki_queries\", \"volumes\", \"apps\"]\n",
    "\n",
    "#     for keyword, filename in zip(keywords, filenames):\n",
    "#         if keyword in tgt or keyword in inp:\n",
    "#             if keyword == \"/u volume\":\n",
    "#                 prev_item = sentence2num(find_tgt(tgt))\n",
    "#             elif keyword == \"/a\":\n",
    "#                 prev_item = find_tgt(inp, \"/a\")\n",
    "#             else:\n",
    "#                 prev_item = find_tgt(tgt)\n",
    "\n",
    "#             if keyword != \"/uvolume\" or (prev_item != \"?\"and prev_item != \"Mute\"):\n",
    "#                 replacement = random.choice(augments[filename])\n",
    "#                 inp = inp.replace(prev_item, replacement)\n",
    "#                 if keyword == \"/a\":\n",
    "#                     prev_item = find_tgt(tgt)\n",
    "\n",
    "#                 tgt = tgt.replace(prev_item, replacement)\n",
    "#                 return inp.replace(\"/a\", \"\"), tgt.replace(\"/a\", \"\")\n",
    "#     return inp, tgt\n",
    "\n",
    "# def get_dataloader(batch_size):\n",
    "#     pairs = prepareData()\n",
    "#     train_pairs, val_pairs = pairs[:225], pairs[225:]\n",
    "\n",
    "#     train_n = len(train_pairs)\n",
    "#     val_n = len(val_pairs)\n",
    "#     train_input_ids = np.zeros((train_n, MAX_LENGTH), dtype=np.int32)\n",
    "#     train_target_ids = np.zeros((train_n, MAX_LENGTH), dtype=np.int32)\n",
    "#     val_input_ids = np.zeros((val_n, MAX_LENGTH), dtype=np.int32)\n",
    "#     val_target_ids = np.zeros((val_n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "#     for idx, (inp, tgt) in enumerate(train_pairs):\n",
    "#         inp, tgt = augment(inp, tgt)\n",
    "#         inp_encoded = tokenizer.encode(inp)\n",
    "#         tgt_ids = tokenizer.encode(tgt)\n",
    "#         train_input_ids[idx, :len(inp_encoded)] = inp_encoded\n",
    "#         train_target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "        \n",
    "#     for idx, (inp, tgt) in enumerate(val_pairs):\n",
    "#         inp, tgt = augment(inp, tgt)\n",
    "#         inp_encoded = tokenizer.encode(inp)\n",
    "#         tgt_ids = tokenizer.encode(tgt)\n",
    "#         val_input_ids[idx, :len(inp_encoded)] = inp_encoded\n",
    "#         val_target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "#     train_data = TensorDataset(torch.LongTensor(train_input_ids).to(device),\n",
    "#                                torch.LongTensor(train_target_ids).to(device))\n",
    "#     val_data = TensorDataset(torch.LongTensor(val_input_ids).to(device),\n",
    "#                                torch.LongTensor(val_target_ids).to(device))\n",
    "\n",
    "#     train_sampler = RandomSampler(train_data)\n",
    "#     train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "#     val_sampler = RandomSampler(val_data)\n",
    "#     val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "    \n",
    "#     return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b50ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"/u shopping\", \"/u todolist\", \"/u wiki\", \"/u volume\", \"/a/\"]\n",
    "filenames = [\"shopping_items\", \"todo_list_items\", \"wiki_queries\", \"volumes\", \"apps\"]\n",
    "augments = {\"shopping_items\": [], \"todo_list_items\": [],\n",
    "            \"wiki_queries\": [], \"volumes\": [], \"apps\": []}\n",
    "\n",
    "for keyword, filename in zip(keywords, filenames):\n",
    "    with open(f\"/media/nathanmon/389E28739E282BB6/Users/Natha/Datasets/MyJarvisConversation/{filename}.txt\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            augments[filename].append(line.replace(\"\\n\", \"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f926c23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, pairs, augments,\n",
    "                 batch_size=16, max_length=150):\n",
    "        self.pairs = pairs\n",
    "        self.augments = augments\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.numbers = [\"zero\", \"one\", \"two\", \"three\", \"four\", \n",
    "           \"five\", \"six\", \"seven\", \"eight\", \"nine\",\n",
    "           \"ten\", \"eleven\", \"twelve\", \"thirteen\",\n",
    "           \"fourteen\", \"fifteen\", \"sixteen\", \n",
    "           \"seventeen\", \"eighteen\", \"nineteen\",\n",
    "           \"twenty\", \"thirty\", \"forty\", \"fifty\",\n",
    "           \"sixty\", \"seventy\", \"eighty\", \"ninety\",\n",
    "           \"hundred\", \"thousand\", \"million\", \"billion\",\n",
    "           \"trillion\", \"quadrillion\", \"quintillion\", \"mute\", \"?\"]\n",
    "        \n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def sentence2num(self, sentence):\n",
    "        words = sentence.split(\" \")\n",
    "        filtered = []\n",
    "        for word in words:\n",
    "            if word.lower() in self.numbers:\n",
    "                filtered.append(word)\n",
    "\n",
    "        return \" \".join(filtered)\n",
    "    \n",
    "    def find_tgt(self, response, loc=\"'\"):\n",
    "        lower = response.index(loc) + len(loc)\n",
    "        upper = response[lower:].index(loc) + lower\n",
    "        return response[lower:upper]\n",
    "    \n",
    "    def augment(self, inp, tgt):\n",
    "        keywords = [\"/u shopping\", \"/u todolist\", \"/u wiki\", \"/u volume\", \"/a\"]\n",
    "        filenames = [\"shopping_items\", \"todo_list_items\", \"wiki_queries\", \"volumes\", \"apps\"]\n",
    "\n",
    "        for keyword, filename in zip(keywords, filenames):\n",
    "            if keyword in tgt or keyword in inp:\n",
    "                if keyword == \"/u volume\":\n",
    "                    prev_item = self.sentence2num(self.find_tgt(tgt))\n",
    "                elif keyword == \"/a\":\n",
    "                    prev_item = self.find_tgt(inp, \"/a\")\n",
    "                else:\n",
    "                    prev_item = self.find_tgt(tgt)\n",
    "                \n",
    "                if keyword != \"/uvolume\" or (prev_item != \"?\"and prev_item.lower() != \"mute\"):\n",
    "                    replacement = random.choice(self.augments[filename])\n",
    "                    inp = inp.replace(prev_item, replacement)\n",
    "                    if keyword == \"/a\":\n",
    "                        prev_item = self.find_tgt(tgt)\n",
    "                        \n",
    "                    tgt = tgt.replace(prev_item, replacement)\n",
    "                    return inp.replace(\"/a\", \"\"), tgt.replace(\"/a\", \"\")\n",
    "        return inp, tgt\n",
    "\n",
    "    def getitem(self, idx, augment=False):\n",
    "        inps_tokenized, inps_types, inps_masked, targs_in, targs_out = [], [], [], [], []\n",
    "        start_idx = idx*self.batch_size\n",
    "        for (inp, tgt) in self.pairs[start_idx:start_idx+batch_size]:\n",
    "            if augment:\n",
    "                inp, tgt = self.augment(inp, tgt)\n",
    "            \n",
    "            inp_encoded = tokenizer.encode_plus(inp,\n",
    "                                                max_length=MAX_LENGTH,\n",
    "                                                pad_to_max_length=True)['input_ids']\n",
    "            tgt_encoded = tokenizer.encode_plus(tgt,\n",
    "                                                max_length=MAX_LENGTH,\n",
    "                                                pad_to_max_length=True)['input_ids']\n",
    "            \n",
    "            targs_in.append(inp_encoded)\n",
    "            targs_out.append(tgt_encoded)\n",
    "            \n",
    "        targs_in = torch.tensor(targs_in).to(device)\n",
    "        targs_out = torch.tensor(targs_out).to(device)\n",
    "        \n",
    "        return targs_in, targs_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f60b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = prepareData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a777856",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TokenizedDataset(pairs, augments, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a686c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = get_dataloader(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcaba16",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2761de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4b6eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d089ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion, train=True):\n",
    "    global batch_size\n",
    "    total_loss = 0\n",
    "#     for data in dataloader:\n",
    "#         input_tensor, target_tensor = data\n",
    "    for batch in range(len(dataloader) // batch_size):\n",
    "        input_tensor, target_tensor = dataloader.getitem(batch, augment=True)\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        if train:\n",
    "            loss.backward()\n",
    "\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4076f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13064dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, val_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_train_losses = []\n",
    "    print_train_loss_total = 0  # Reset every print_every\n",
    "    plot_train_loss_total = 0  # Reset every plot_every\n",
    "    \n",
    "    print_val_loss_total = 0  # Reset every print_every\n",
    "    plot_val_loss_total = 0\n",
    "    plot_val_losses = []\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_train_loss_total += train_loss\n",
    "        plot_train_loss_total += train_loss\n",
    "        \n",
    "        # Evaluate validation dataloader\n",
    "        val_loss = train_epoch(val_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, train=False)\n",
    "        print_val_loss_total += val_loss\n",
    "        plot_val_loss_total += val_loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_train_loss_avg = print_train_loss_total / print_every\n",
    "            print_train_loss_total = 0\n",
    "            print_val_loss_avg = print_val_loss_total / print_every\n",
    "            print_val_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_train_loss_avg, print_val_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_train_loss_avg = plot_train_loss_total / plot_every\n",
    "            plot_train_losses.append(plot_train_loss_avg)\n",
    "            plot_train_loss_total = 0\n",
    "            plot_val_loss_avg = plot_val_loss_total / plot_every\n",
    "            plot_val_losses.append(plot_val_loss_avg)\n",
    "            plot_val_loss_total = 0\n",
    "\n",
    "    showPlot(plot_train_losses, \"loss\", plot_val_losses, \"val_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points, points_name, points2=None, points2_name=None):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.5)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    if points2 != None:\n",
    "        plt.plot(np.arange(len(points)), points, points2)\n",
    "        plt.legend([points_name, points2_name])\n",
    "    else:\n",
    "        plt.plot(points)\n",
    "        plt.legend([points_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2558ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor(tokenizer.encode(sentence), dtype=torch.long).to(\"cuda\").view(1, -1)\n",
    "        \n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(tokenizer.convert_ids_to_tokens(idx.item()))\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929b34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085de9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "batch_size = 16\n",
    "dropout_p = 0.7\n",
    "\n",
    "# train_dataloader, val_dataloader = get_dataloader(batch_size)\n",
    "train_dataloader = TokenizedDataset(pairs[:int(.9 * len(pairs))], augments, \n",
    "                                   batch_size=batch_size)\n",
    "val_dataloader = TokenizedDataset(pairs[int(.9 * len(pairs)):], augments, \n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "encoder = EncoderRNN(len(tokenizer.get_vocab()), hidden_size, dropout_p=dropout_p).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, len(tokenizer.get_vocab()), dropout_p=dropout_p).to(device)\n",
    "\n",
    "train(train_dataloader, val_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d337bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder, \"encoder.pth\")\n",
    "torch.save(decoder, \"decoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5d6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones_place = {\"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \n",
    "              \"five\": 5, \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9,\n",
    "              \"ten\": 10, \"eleven\": 11, \"twelve\": 12, \"thirteen\": 13,\n",
    "              \"fourteen\": 14, \"fifteen\": 15, \"sixteen\": 16, \n",
    "              \"seventeen\": 17, \"eighteen\": 18, \"nineteen\": 19}\n",
    "\n",
    "tens_place = {\"twenty\": 20, \"thirty\": 30, \"forty\": 40, \"fifty\": 50,\n",
    "              \"sixty\": 60, \"seventy\": 70, \"eighty\": 80, \"ninety\": 90}\n",
    "\n",
    "hundreds_place = {\"hundred\": 10**2, \"thousand\": 10**3, \"million\": 10**6, \"billion\": 10**9,\n",
    "                  \"trillion\": 10**12, \"quadrillion\": 10**15, \"quintillion\": 10**18}\n",
    "\n",
    "# Convert word form to numbers\n",
    "# Works for numbers up to the quintillions\n",
    "# Also works for dates like nineteen sixty three of twenty twenty four\n",
    "def word2num(word):\n",
    "    digits = word.split()\n",
    "    num = 0\n",
    "    \n",
    "    has_ones = False\n",
    "    has_tens = False\n",
    "    tens = 0\n",
    "    hundreds = 0\n",
    "    for i in range(len(digits)):\n",
    "        digit = digits[i]\n",
    "        \n",
    "        if digit in ones_place:\n",
    "            # In a real number, there would never be a ones place or tens place next to another ones place\n",
    "            # For example, there is never nine nineteen or four ninety\n",
    "            # However, this is seen in a year such as nineteen forty one\n",
    "            if (has_tens and ones_place[digit] >= 10) or has_ones:\n",
    "                num *= 100\n",
    "                \n",
    "            num += ones_place[digit]\n",
    "            # Store the tens place and hundreds place in case there is a thousand or million in front of them\n",
    "            tens += ones_place[digit]\n",
    "            hundreds += ones_place[digit]\n",
    "            # Store boolean values to see if there has been a tens place or ones place\n",
    "            has_ones = True\n",
    "            has_tens = False\n",
    "    \n",
    "        elif digit in tens_place:\n",
    "            if has_ones or has_tens:\n",
    "                # If someone enters a year(eighteen twelve works different than one thousand eight hundred twelve)\n",
    "                num *= 100\n",
    "                has_tens = False\n",
    "            else:\n",
    "                has_tens = True\n",
    "            \n",
    "            num += tens_place[digit]\n",
    "            tens += tens_place[digit]\n",
    "            hundreds += tens_place[digit]\n",
    "            has_ones = False\n",
    "            \n",
    "        elif digit in hundreds_place:\n",
    "            if digit != \"hundred\" and hundreds:\n",
    "                num += hundreds * hundreds_place[digit] - hundreds\n",
    "                hundreds = 0\n",
    "                tens = 0\n",
    "            else:\n",
    "                num += tens * hundreds_place[digit] - tens\n",
    "                hundreds = tens * hundreds_place[digit]\n",
    "                tens = 0\n",
    "            has_ones = False\n",
    "            has_tens = False\n",
    "        \n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf92f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\"\n",
    "}\n",
    "def weather(city, wob):\n",
    "    res = requests.get(\n",
    "        f'https://www.google.com/search?q={city}+weather&rlz=1C1VDKB_enUS1016US1016&oq={city}&aqs=chrome.0.69i59j69i57j69i59l2j0i271l2j69i61l2.905j0j7&sourceid=chrome&ie=UTF-8',\n",
    "        headers=headers\n",
    "    )\n",
    "    soup = bs4(res.text, 'html.parser')\n",
    "    return soup.select(f\"#wob_{wob}\")[0].getText().strip()\n",
    "\n",
    "def volume(vol):\n",
    "    devices = AudioUtilities.GetSpeakers()\n",
    "    interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "    volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "\n",
    "    volume.SetMasterVolumeLevelScalar(word2num(vol)/100, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    chat = input(\"Speak: \")\n",
    "    output = evaluate(encoder, decoder, chat)\n",
    "    response = ' '.join(output[0][:-1])\n",
    "    \n",
    "    print(\"Response: \", response)\n",
    "    # Get the date\n",
    "    if \"/udate\" in response:\n",
    "        response = response.replace(\"/udate\", datetime.date.today().strftime(\"%B %d, %Y\"))\n",
    "    # Get the time\n",
    "    if \"/utime\" in response:\n",
    "        response = response.replace(\"/utime\", datetime.datetime.now().strftime(\"%I:%M:%S\"))\n",
    "    # Get the temperature\n",
    "    if \"/utemp\" in response:\n",
    "        response = response.replace(\"/utemp\", weather(\"boston\", \"tm\"))\n",
    "    # Get the humidity\n",
    "    if \"/uhumidity\" in response:\n",
    "        response = response.replace(\"/uhumidity\", weather(\"boston\", \"hm\"))\n",
    "    # Get the wind speed\n",
    "    if \"/uwind\" in response:\n",
    "        response = response.replace(\"/uwind\", weather(\"boston\", \"ws\"))\n",
    "    # Get the amount of precipitation\n",
    "    if \"/uprecipitation\" in response:\n",
    "        response = response.replace(\"/uprecipitation\", weather(\"boston\", \"pp\"))\n",
    "    if \"/uvolume\" in response:\n",
    "        after = response.split(\"/uvolume\")[-1]\n",
    "        vol = after.split(\"'\")[1]\n",
    "        #response = response.replace(\"/uvolume\"+\"'\"+vol+\"'\", \"\")\n",
    "        volume(vol)\n",
    "    if \"/usleep\" in response:\n",
    "        response = response.replace(\"/usleep\", \"\")\n",
    "        print(response)\n",
    "        break\n",
    "    if \"/unewtab\" in response:\n",
    "        response = response.replace(\"/unewtab\", \"\")\n",
    "        pyautogui.hotkey('ctrl', 't')\n",
    "    if \"/uclosetab\" in response:\n",
    "        response = response.replace(\"/uclosetab\", \"\")\n",
    "        pyautogui.hotkey('ctrl', 'w')\n",
    "    if \"/uswitchtab\" in response:\n",
    "        after = response.split(\"/uvolume\")[-1]\n",
    "        new = after.split(\"'\")[1]\n",
    "        response = response.replace(\"/uswitchtab\"+\"'\"+new+\"'\", \"\")\n",
    "        pyautogui.hotkey('ctrl', str(word2num(new)))\n",
    "    \n",
    "    print(\"Filtered: \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d3b56a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
