{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5837cec5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd43b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models.decoder import download_pretrained_files\n",
    "from torchaudio.models.decoder import ctc_decoder\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from TTS.api import TTS\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import screen_brightness_control as sbc\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "from playsound import playsound\n",
    "from subprocess import call\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import pyautogui\n",
    "import datetime\n",
    "import keyboard\n",
    "import requests\n",
    "import pyaudio\n",
    "import random\n",
    "import psutil\n",
    "import time\n",
    "import wave\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77089ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992b4aba",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91698a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "OOV_token = 2\n",
    "\n",
    "class Lang(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"OOV\", 3: \"PAD\"}\n",
    "        self.n_words = 3  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73874b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z'/!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"'\", \" '\", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b403737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    words = []\n",
    "    for word in normalizeString(sentence).split(' '):\n",
    "        if word in lang.word2index:\n",
    "            words.append(lang.word2index[word])\n",
    "        else:\n",
    "            words.append(OOV_token)\n",
    "    return words\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20329857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6526d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ce2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6355c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_nn = torch.load(\"encoder.pth\")\n",
    "decoder_nn = torch.load(\"decoder.pth\")\n",
    "input_lang = torch.load(\"input_lang.pth\")\n",
    "output_lang = torch.load(\"output_lang.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a1d0f9",
   "metadata": {},
   "source": [
    "# Voice entity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c076ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActDropNormCNN1D(nn.Module):\n",
    "    def __init__(self, n_feats, dropout, keep_shape=False):\n",
    "        super(ActDropNormCNN1D, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(n_feats)\n",
    "        self.keep_shape = keep_shape\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        # x = self.norm(self.dropout(F.gelu(x)))\n",
    "        x = self.dropout(F.gelu(self.norm(x)))\n",
    "        if self.keep_shape:\n",
    "            return x.transpose(1, 2)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class VoiceEntityClassification(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes, n_feats, num_layers, dropout):\n",
    "        super(VoiceEntityClassification, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(n_feats, n_feats, 10, 2, padding=10//2),\n",
    "            ActDropNormCNN1D(n_feats, dropout, keep_shape=True),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(n_feats, n_feats, 10, 2, padding=10//2),\n",
    "            ActDropNormCNN1D(n_feats, dropout),\n",
    "        )\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(n_feats*267, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        self.layer_norm2 = nn.LayerNorm(128)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.final_fc = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        n, hs = self.num_layers, self.hidden_size\n",
    "        return (torch.zeros(n*1, batch_size, hs).to(device),\n",
    "                torch.zeros(n*1, batch_size, hs).to(device)\n",
    "               )\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = x.to(device)\n",
    "        x = self.cnn1(x) # batch, channels, time, feature\n",
    "        x = self.cnn2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dense(x) # batch, time, feature\n",
    "        x = self.dropout2(F.gelu(self.layer_norm2(x)))  # (time, batch, n_class)\n",
    "        return self.final_fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e35aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_voice = torch.load(\"my_voice.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fd030e",
   "metadata": {},
   "source": [
    "# Voice target classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb09062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, emb_size, num_layers, dropout_p=0.1):\n",
    "        super(TargetClassifier, self).__init__()\n",
    "#         self.positional_encoding = PositionalEncoding(emb_size, dropout_p)\n",
    "        self.gru_layers = nn.ModuleList([nn.GRU(emb_size if i == 0 else hidden_size,\n",
    "                                       hidden_size,\n",
    "                                       batch_first=True)\n",
    "                                       for i in range(num_layers)])\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layernorm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "#         embedded = self.positional_encoding(x)\n",
    "    \n",
    "        for gru_layer in self.gru_layers:\n",
    "            x, _ = gru_layer(x)\n",
    "            \n",
    "        x = x[:, -1, :]\n",
    "        x = self.dense(x)\n",
    "        x = self.dropout(F.gelu(self.layernorm(x)))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db590208",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_target = torch.load(\"target_recognition.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd86b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoder = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c830ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeString(text, tokenizer, encoder):\n",
    "    indexed = tokenizer.encode_plus(text,\n",
    "                                    max_length=20,\n",
    "                                    pad_to_max_length=True,\n",
    "                                    return_attention_mask=True,\n",
    "                                    return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encodings = encoder(**indexed)\n",
    "        \n",
    "    last_hidden_states = encodings.last_hidden_state\n",
    "    return last_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c41dab",
   "metadata": {},
   "source": [
    "# Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6c2a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "acoustic_model = bundle.get_model()\n",
    "\n",
    "files = download_pretrained_files(\"librispeech-4-gram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13522499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acoustic_model = torch.load(\"my_speech_recognition.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_WEIGHT = 3\n",
    "WORD_SCORE = -0.26\n",
    "\n",
    "beam_search_decoder = ctc_decoder(\n",
    "    lexicon=files.lexicon,\n",
    "    tokens=files.tokens,\n",
    "    lm=files.lm,\n",
    "    nbest=3,\n",
    "    beam_size=100,\n",
    "    lm_weight=LM_WEIGHT,\n",
    "    word_score=WORD_SCORE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287df27",
   "metadata": {},
   "source": [
    "# Voice Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TTS().list_models())\n",
    "\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844fb90b",
   "metadata": {},
   "source": [
    "# Necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7885dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones_place = {\"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \n",
    "              \"five\": 5, \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9,\n",
    "              \"ten\": 10, \"eleven\": 11, \"twelve\": 12, \"thirteen\": 13,\n",
    "              \"fourteen\": 14, \"fifteen\": 15, \"sixteen\": 16, \n",
    "              \"seventeen\": 17, \"eighteen\": 18, \"nineteen\": 19}\n",
    "\n",
    "tens_place = {\"twenty\": 20, \"thirty\": 30, \"forty\": 40, \"fifty\": 50,\n",
    "              \"sixty\": 60, \"seventy\": 70, \"eighty\": 80, \"ninety\": 90}\n",
    "\n",
    "hundreds_place = {\"hundred\": 10**2, \"thousand\": 10**3, \"million\": 10**6, \"billion\": 10**9,\n",
    "                  \"trillion\": 10**12, \"quadrillion\": 10**15, \"quintillion\": 10**18}\n",
    "\n",
    "# Convert word form to numbers\n",
    "# Works for numbers up to the quintillions\n",
    "# Also works for dates like nineteen sixty three of twenty twenty four\n",
    "def word2num(word):\n",
    "    digits = word.split()\n",
    "    num = 0\n",
    "    \n",
    "    has_ones = False\n",
    "    has_tens = False\n",
    "    tens = 0\n",
    "    hundreds = 0\n",
    "    for i in range(len(digits)):\n",
    "        digit = digits[i]\n",
    "        \n",
    "        if digit in ones_place:\n",
    "            # In a real number, there would never be a ones place or tens place next to another ones place\n",
    "            # For example, there is never nine nineteen or four ninety\n",
    "            # However, this is seen in a year such as nineteen forty one\n",
    "            if (has_tens and ones_place[digit] >= 10) or has_ones:\n",
    "                num *= 100\n",
    "                \n",
    "            num += ones_place[digit]\n",
    "            # Store the tens place and hundreds place in case there is a thousand or million in front of them\n",
    "            tens += ones_place[digit]\n",
    "            hundreds += ones_place[digit]\n",
    "            # Store boolean values to see if there has been a tens place or ones place\n",
    "            has_ones = True\n",
    "            has_tens = False\n",
    "    \n",
    "        elif digit in tens_place:\n",
    "            if has_ones or has_tens:\n",
    "                # If someone enters a year(eighteen twelve works different than one thousand eight hundred twelve)\n",
    "                num *= 100\n",
    "                has_tens = False\n",
    "            else:\n",
    "                has_tens = True\n",
    "            \n",
    "            num += tens_place[digit]\n",
    "            tens += tens_place[digit]\n",
    "            hundreds += tens_place[digit]\n",
    "            has_ones = False\n",
    "            \n",
    "        elif digit in hundreds_place:\n",
    "            if digit != \"hundred\" and hundreds:\n",
    "                num += hundreds * hundreds_place[digit] - hundreds\n",
    "                hundreds = 0\n",
    "                tens = 0\n",
    "            else:\n",
    "                num += tens * hundreds_place[digit] - tens\n",
    "                hundreds = tens * hundreds_place[digit]\n",
    "                tens = 0\n",
    "            has_ones = False\n",
    "            has_tens = False\n",
    "        \n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df382c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\"\n",
    "}\n",
    "def weather(city, wob):\n",
    "    res = requests.get(\n",
    "        f'https://www.google.com/search?q={city}+weather&rlz=1C1VDKB_enUS1016US1016&oq={city}&aqs=chrome.0.69i59j69i57j69i59l2j0i271l2j69i61l2.905j0j7&sourceid=chrome&ie=UTF-8',\n",
    "        headers=headers\n",
    "    )\n",
    "    soup = bs4(res.text, 'html.parser')\n",
    "    return soup.select(f\"#wob_{wob}\")[0].getText().strip()\n",
    "\n",
    "def volume(vol):\n",
    "    if \"increase\" in vol:\n",
    "        call([\"amixer\", \"-D\", \"pulse\", \"sset\", \"Master\", f\"{word2num(vol)}%+\"])\n",
    "    elif \"decrease\" in vol:\n",
    "        call([\"amixer\", \"-D\", \"pulse\", \"sset\", \"Master\", f\"{word2num(vol)}%-\"])\n",
    "    elif \"mute\" in vol:\n",
    "        call([\"amixer\", \"-q\", \"-D\", \"pulse\", \"sset\", \"Master\", \"toggle\"])\n",
    "    else:\n",
    "        f = call([\"amixer\", \"-D\", \"pulse\", \"sset\", \"Master\", f\"{word2num(vol)}%\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14669f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = 1024\n",
    "sample_format = pyaudio.paInt16\n",
    "channels = 1\n",
    "fs = 16000\n",
    "seconds = 5\n",
    "filename = \"temp.wav\"\n",
    "frames = []\n",
    "\n",
    "p = pyaudio.PyAudio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bd0ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(waveform):\n",
    "    wf = wave.open(filename, \"wb\")\n",
    "    # set the channels\n",
    "    wf.setnchannels(1)\n",
    "    # set the sample format\n",
    "    wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "    # set the sample rate\n",
    "    wf.setframerate(16000)\n",
    "    # write the frames as bytes\n",
    "    wf.writeframes(b\"\".join(waveform))\n",
    "    # close the file\n",
    "    wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ce74cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google(command):\n",
    "    if command == \"open\":\n",
    "        if \"chrome\" in (i.name() for i in psutil.process_iter()): # Chrome is already open\n",
    "            # Create a new separate window\n",
    "            pyautogui.hotkey('ctrl', 'n')\n",
    "        else: # If google is not open\n",
    "            # Open chrome\n",
    "            pyautogui.press(\"winleft\")\n",
    "            pyautogui.typewrite(\"google chrome\")\n",
    "            pyautogui.press(\"enter\")\n",
    "    elif command == \"close\":\n",
    "        pyautogui.hotkey(\"Alt\", \"f\")\n",
    "        pyautogui.press(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3583c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anaconda(command):\n",
    "    # Anaconda is not currently used on this device\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f060c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(command):\n",
    "    time = word2num(command)\n",
    "    desired_seconds = time\n",
    "    \n",
    "    if \"minute\" in command:\n",
    "        desired_seconds = time * 60\n",
    "    if \"hour\" in command:\n",
    "        desired_seconds = time * 3600\n",
    "    return desired_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b0b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(chat):\n",
    "    output = evaluate(encoder_nn, decoder_nn, chat, input_lang, output_lang)\n",
    "    response = ' '.join(output[0][:-1])\n",
    "    desired_seconds = None\n",
    "    print(response)\n",
    "    \n",
    "    # Get the date\n",
    "    if \"/udate\" in response:\n",
    "        response = response.replace(\"/udate\", datetime.date.today().strftime(\"%B %d, %Y\"))\n",
    "    # Get the time\n",
    "    if \"/utime\" in response:\n",
    "        response = response.replace(\"/utime\", datetime.datetime.now().strftime(\"%I:%M:%S\"))\n",
    "    # Get the temperature\n",
    "    if \"/utemp\" in response:\n",
    "        response = response.replace(\"/utemp\", weather(\"boston\", \"tm\"))\n",
    "    # Get the humidity\n",
    "    if \"/uhumidity\" in response:\n",
    "        response = response.replace(\"/uhumidity\", weather(\"boston\", \"hm\"))\n",
    "    # Get the wind speed\n",
    "    if \"/uwind\" in response:\n",
    "        response = response.replace(\"/uwind\", weather(\"boston\", \"ws\"))\n",
    "    # Get the amount of precipitation\n",
    "    if \"/uprecipitation\" in response:\n",
    "        response = response.replace(\"/uprecipitation\", weather(\"boston\", \"pp\"))\n",
    "    if \"/uvolume\" in response:\n",
    "        after = response.split(\"/uvolume\")[-1]\n",
    "        vol = after.split(\"'\")[1]\n",
    "        response = response.replace(\"/uvolume\"+\"  '\"+vol+\"  '\", \"\")\n",
    "        volume(vol)\n",
    "    if \"/usleep\" in response:\n",
    "        response = response.replace(\"/usleep\", \"\")\n",
    "        return response\n",
    "    if \"/unewtab\" in response:\n",
    "        response = response.replace(\"/unewtab\", \"\")\n",
    "        pyautogui.hotkey('ctrl', 't')\n",
    "    if \"/uclosetab\" in response:\n",
    "        response = response.replace(\"/uclosetab\", \"\")\n",
    "        pyautogui.hotkey('ctrl', 'w')\n",
    "    if \"/uswitchtab\" in response:\n",
    "        after = response.split(\"/uswitchtab\")[-1]\n",
    "        new = after.split(\"'\")[1]\n",
    "        response = response.replace(\"/uswitchtab\"+\"  '\"+new+\"  '\", \"\")\n",
    "        pyautogui.hotkey('ctrl', str(word2num(new)))\n",
    "    if \"/ugoogle\" in response:\n",
    "        after = response.split(\"/ugoogle\")[-1]\n",
    "        command = after.split(\"'\")[1]\n",
    "        response = response.replace(\"/ugoogle\"+\"  '\"+command+\"  '\", \"\")\n",
    "        google(command)\n",
    "    if \"/uanaconda\" in response:\n",
    "        after = response.split(\"/ugoogle\")[-1]\n",
    "        command = after.split(\"'\")[1]\n",
    "        response = response.replace(\"/ugoogle\"+\"  '\"+command+\"  '\", \"\")\n",
    "        anaconda(command)\n",
    "    if \"/utimer\" in response:\n",
    "        after = response.split(\"/utimer\")[-1]\n",
    "        command = after.split(\"'\")[1]\n",
    "        response = response.replace(\"/utimer\"+\"  '\"+command+\"  '\", \"\")\n",
    "        desired_seconds = timer(command)\n",
    "        \n",
    "    tts.tts_to_file(text=response, \\\n",
    "            speaker_wav=\"jarvis_speech_files/killing.wav\", language=\"en\", file_path=\"output.wav\")\n",
    "    playsound('output.wav')\n",
    "\n",
    "    return response, desired_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8754d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 128\n",
    "win_length = 160\n",
    "hop_length = 80\n",
    "max_length = 85000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78befe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_waveform(filename):\n",
    "    waveform, sample_rate = torchaudio.load(filename)\n",
    "\n",
    "    # Check sample rate\n",
    "    if sample_rate != bundle.sample_rate:\n",
    "        waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a15d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_waveform(waveform, max_length, bundle, n_mels, win_length, hop_length):\n",
    "    if len(waveform[0]) < max_length:\n",
    "        padded_wav = torch.concat((waveform[0], torch.zeros(max_length - len(waveform[0])))).unsqueeze(-2)\n",
    "    else:\n",
    "        cut_length = len(waveform[0]) - max_length\n",
    "        padded_wav = waveform[0][cut_length//2:len(waveform[0])-(cut_length//2)].unsqueeze(-2)\n",
    "        \n",
    "    spect = torchaudio.transforms.MelSpectrogram(\n",
    "                                sample_rate=bundle.sample_rate, n_mels=n_mels,\n",
    "                                win_length=win_length, \n",
    "                                hop_length=hop_length)(padded_wav)\n",
    "    spect = np.log(spect + 1e-14)\n",
    "        \n",
    "    return spect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aee779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_audio(filename):\n",
    "    os.remove(filename)\n",
    "    time.sleep(3)\n",
    "    return [], 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a21f04",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e7119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import keyboard\n",
    "print('Recording')\n",
    "\n",
    "stream = p.open(format=sample_format,\n",
    "                channels=channels,\n",
    "                rate=fs,\n",
    "                frames_per_buffer=chunk,\n",
    "                input=True)\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    os.remove(filename)\n",
    "frames = []\n",
    "\n",
    "desired_seconds = None\n",
    "prev_seconds = None\n",
    "\n",
    "counter = 0\n",
    "periodically_check = False\n",
    "transcript_placeholder = \"\"\n",
    "speaking_counter = 0\n",
    "check_after = 3\n",
    "while True:\n",
    "    if len(frames) > seconds * fs / chunk:\n",
    "        for i in range(int(len(frames) - (seconds * fs / chunk))):\n",
    "            frames.pop(0)\n",
    "        \n",
    "    data = stream.read(chunk, exception_on_overflow=False)\n",
    "    frames.append(data)\n",
    "    \n",
    "    save(frames)\n",
    "    if counter % 10 == 0:\n",
    "        waveform = load_waveform(filename) # Load wave\n",
    "        spect = pad_waveform(waveform, max_length, bundle, n_mels, \n",
    "                             win_length, hop_length) # Pad wave(for my voice classification)\n",
    "        \n",
    "        # Automatic Speech Recognition\n",
    "        emission, _ = acoustic_model(waveform)\n",
    "        beam_search_result = beam_search_decoder(emission)\n",
    "        beam_search_transcript = \" \".join(beam_search_result[0][0].words).strip()\n",
    "        \n",
    "        # Check if still speaking\n",
    "        if periodically_check:            \n",
    "            if len(transcript_placeholder) >= len(beam_search_transcript): # if transcipt is no longer increasing\n",
    "                if beam_search_result[0][0].words != []:\n",
    "                    if speaking_counter == check_after: # if there's been a long enough pause\n",
    "                        periodically_check = False\n",
    "                        print(\"End of speaking\")\n",
    "                        # Check if it's my voice\n",
    "                        if my_voice(spect, my_voice._init_hidden(1)):\n",
    "                            # Check if I am talking to Jarvis\n",
    "                            speech = encodeString(beam_search_transcript, tokenizer, encoder)\n",
    "                            out = voice_target(speech)\n",
    "                            target = F.sigmoid(out)\n",
    "\n",
    "                            if torch.round(target) == 1:\n",
    "                                print(\"Input: \", beam_search_transcript)\n",
    "                                response, desired_seconds = respond(beam_search_transcript)\n",
    "                                print(\"Response: \", response)\n",
    "                                \n",
    "                                # If a timer is created, set a timestamp\n",
    "                                if desired_seconds != None:\n",
    "                                    prev_seconds = time.time()\n",
    "                                \n",
    "                            else:\n",
    "                                print(\"Not adressing Jarvis\")\n",
    "                        else:\n",
    "                            print(\"Not my voice\")\n",
    "                        frames, speaking_counter = reset_audio(filename)\n",
    "                        transcript_placeholder = \"\"\n",
    "\n",
    "                    elif speaking_counter < check_after:\n",
    "                        # If the length of the speaking has gone down, increment a counter\n",
    "                        speaking_counter += 1\n",
    "            else: # The transcript is still increasing\n",
    "                speaking_counter = 0\n",
    "                transcript_placeholder = beam_search_transcript\n",
    "        \n",
    "        # First detect speaking\n",
    "        if beam_search_result[0][0].words != [] and not periodically_check:\n",
    "            print(\"Speaking detected\")\n",
    "            periodically_check = True\n",
    "            transcript_placeholder = beam_search_transcript\n",
    "#             frames = []\n",
    "#             os.remove(filename)\n",
    "        \n",
    "        print(\"Input: \", beam_search_transcript)\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "    counter += 1\n",
    "    \n",
    "    if desired_seconds != None and round(time.time() - prev_seconds) == desired_seconds:\n",
    "        # playsound(\"timer\")\n",
    "        print(\"Timer ended\")\n",
    "        pass\n",
    "    \n",
    "# Stop and close the stream \n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "# Terminate the PortAudio interface\n",
    "p.terminate()\n",
    "print(\"Program terminated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb00509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "# import keyboard\n",
    "# print('Recording')\n",
    "\n",
    "# stream = p.open(format=sample_format,\n",
    "#                 channels=channels,\n",
    "#                 rate=fs,\n",
    "#                 frames_per_buffer=chunk,\n",
    "#                 input=True)\n",
    "\n",
    "# if os.path.exists(filename):\n",
    "#     os.remove(filename)\n",
    "# frames = []\n",
    "\n",
    "# counter = 0\n",
    "# periodically_check = False\n",
    "# transcript_placeholder = \"\"\n",
    "# speaking_counter = 0\n",
    "# check_after = 3\n",
    "# while True:\n",
    "#     if len(frames) > seconds * fs / chunk:\n",
    "#         for i in range(int(len(frames) - (seconds * fs / chunk))):\n",
    "#             frames.pop(0)\n",
    "        \n",
    "#     data = stream.read(chunk, exception_on_overflow=False)\n",
    "#     frames.append(data)\n",
    "    \n",
    "#     save(frames)\n",
    "#     if counter % 10 == 0:\n",
    "#         waveform, sample_rate = torchaudio.load(filename)\n",
    "\n",
    "#         # Check sample rate\n",
    "#         if sample_rate != bundle.sample_rate:\n",
    "#             waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "        \n",
    "#         # Pad or slice waveform(for detecting my voice)\n",
    "#         if len(waveform[0]) < max_length:\n",
    "#             padded_wav = torch.concat((waveform[0], torch.zeros(max_length - len(waveform[0])))).unsqueeze(-2)\n",
    "#         else:\n",
    "#             cut_length = len(waveform[0]) - max_length\n",
    "#             padded_wav = waveform[0][cut_length//2:len(waveform[0])-(cut_length//2)].unsqueeze(-2)\n",
    "            \n",
    "#         spect = torchaudio.transforms.MelSpectrogram(\n",
    "#                                     sample_rate=bundle.sample_rate, n_mels=n_mels,\n",
    "#                                     win_length=win_length, \n",
    "#                                     hop_length=hop_length)(padded_wav)\n",
    "#         spect = np.log(spect + 1e-14)\n",
    "#         emission, _ = acoustic_model(waveform)\n",
    "#         beam_search_result = beam_search_decoder(emission)\n",
    "#         beam_search_transcript = \" \".join(beam_search_result[0][0].words).strip()\n",
    "        \n",
    "#         # Check if still speaking\n",
    "#         if periodically_check:            \n",
    "#             if len(transcript_placeholder) >= len(beam_search_transcript):\n",
    "#                 if beam_search_result[0][0].words != []:\n",
    "#                     if speaking_counter == check_after:\n",
    "#                         periodically_check = False\n",
    "#                         print(\"End of speaking\")\n",
    "#                         # Check if it's my voice\n",
    "#                         if my_voice(spect, my_voice._init_hidden(1)):\n",
    "#                             # Check if I am talking to Jarvis\n",
    "#                             speech = encodeString(beam_search_transcript, tokenizer, encoder)\n",
    "#                             out = voice_target(speech)\n",
    "#                             target = F.sigmoid(out)\n",
    "\n",
    "#                             if torch.round(target) == 1:\n",
    "#                                 print(\"Input: \", beam_search_transcript)\n",
    "#                                 print(\"Response: \", respond(beam_search_transcript))\n",
    "#                                 os.remove(filename)\n",
    "#                                 frames = []\n",
    "#                                 time.sleep(3)\n",
    "#                     elif speaking_counter < check_after:\n",
    "#                         # If the length of the speaking has gone down, increment a counter\n",
    "#                         speaking_counter += 1\n",
    "#             else:\n",
    "#                 speaking_counter = 0\n",
    "#                 transcript_placeholder = beam_search_transcript\n",
    "        \n",
    "#         # First detect speaking\n",
    "#         if beam_search_result[0][0].words != [] and not periodically_check:\n",
    "#             periodically_check = True\n",
    "#             transcript_placeholder = beam_search_transcript\n",
    "# #             frames = []\n",
    "# #             os.remove(filename)\n",
    "        \n",
    "#         print(\"Input: \", beam_search_transcript)\n",
    "#         clear_output(wait=True)\n",
    "        \n",
    "#     counter += 1\n",
    "    \n",
    "# # Stop and close the stream \n",
    "# stream.stop_stream()\n",
    "# stream.close()\n",
    "# # Terminate the PortAudio interface\n",
    "# p.terminate()\n",
    "# print(\"Program terminated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f08ac60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
