{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a4b07f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44db65c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models.decoder import download_pretrained_files\n",
    "from torchaudio.models.decoder import ctc_decoder\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from pytorch_model_summary import summary\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio.functional as AF\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from jiwer import wer, cer\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8890dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec5587",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bcb2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_speech_files = glob.glob(\"/media/nathanmon/389E28739E282BB6/Users/Natha/Datasets/MySpeechData/my_voice/*\")\n",
    "other_speech_files = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"test\")['path']\n",
    "music_files = glob.glob(\"/home/nathanmon/Artificial Intelligence/PyTorch/Audio Recognition/Voice Classification/batches/*\")\n",
    "other_speech_files = other_speech_files[:len(my_speech_files)-len(music_files)]\n",
    "\n",
    "positive_labels = np.ones(len(my_speech_files))\n",
    "negative_labels = np.zeros(len(my_speech_files))\n",
    "\n",
    "speech_files = np.concatenate((my_speech_files, other_speech_files, music_files))\n",
    "labels = np.concatenate((positive_labels, negative_labels))\n",
    "\n",
    "combined = list(zip(speech_files, labels))\n",
    "for i in range(5):\n",
    "    np.random.shuffle(combined)\n",
    "\n",
    "speech_files, labels = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ce644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_rate = 8000\n",
    "# max_length = 85000\n",
    "\n",
    "# count = 0\n",
    "# for i, file in enumerate(my_speech_files):\n",
    "#     wav, sr = torchaudio.load(file)\n",
    "#     wav = AF.resample(wav, sr, sample_rate)\n",
    "    \n",
    "#     if len(wav[0]) > 85000:\n",
    "#         count += 1\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be9267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An integer scalar Tensor. The window length in samples.\n",
    "n_mels = 128\n",
    "# An integer scalar Tensor. The number of samples to step.\n",
    "win_length = 160\n",
    "# An integer scalar Tensor. The size of the FFT to apply.\n",
    "# If not provided, uses the smallest power of 2 enclosing frame_length.\n",
    "hop_length = 80\n",
    "\n",
    "def add_noise(waveform, noise_level=0.005):\n",
    "    # Generate random noise with the same length as the waveform\n",
    "    noise = torch.randn_like(waveform) * noise_level\n",
    "    \n",
    "    # Add the noise to the waveform\n",
    "    noisy_waveform = waveform + noise\n",
    "    \n",
    "    return noisy_waveform\n",
    "\n",
    "def load_wav(filename):\n",
    "    wav, sr = torchaudio.load(filename)\n",
    "    wav = AF.resample(wav, sr, sample_rate)\n",
    "    if len(wav[0]) < max_length:\n",
    "        wav = torch.concat((wav[0], torch.zeros(max_length - len(wav[0])))).unsqueeze(-2)\n",
    "    else:\n",
    "        cut_length = len(wav[0]) - max_length\n",
    "        wav = wav[0][cut_length//2:len(wav[0])-(cut_length//2)].unsqueeze(-2)\n",
    "        \n",
    "    noise = np.random.normal(0, 1, max_length)\n",
    "    wav = torchaudio.transforms.Vol(gain=torch.rand(1)/2, gain_type=\"amplitude\")(wav)\n",
    "    wav = add_noise(wav, noise_level=0.02)\n",
    "    return wav, sr\n",
    "\n",
    "def create_spect(wav, sr, freq_mask=15, time_mask=35):\n",
    "    spect = torchaudio.transforms.MelSpectrogram(\n",
    "                                    sample_rate=sr, n_mels=n_mels,\n",
    "                                    win_length=win_length, \n",
    "                                    hop_length=hop_length\n",
    "    )(wav)\n",
    "    spect = nn.Sequential(\n",
    "            torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask),\n",
    "            torchaudio.transforms.TimeMasking(time_mask_param=time_mask),\n",
    "            torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask),\n",
    "            torchaudio.transforms.TimeMasking(time_mask_param=time_mask),\n",
    "#             torchaudio.transforms.TimeStretch(fixed_rate=0.5)\n",
    "    )(spect)\n",
    "    spect = np.log(spect + 1e-14)\n",
    "    \n",
    "    return spect\n",
    "\n",
    "def encode_sample(file, label):\n",
    "    wav, sr = load_wav(file)\n",
    "    spect = create_spect(wav, sr)\n",
    "        \n",
    "    return spect, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb3781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5dcda0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "file = other_speech_files[index]\n",
    "print(\"Speech file: \\n\", file)\n",
    "\n",
    "ex_wav, sr = load_wav(file)\n",
    "ex_spect = create_spect(ex_wav, sr)[0]\n",
    "print(\"Sample rate: \\n\", sr)\n",
    "print(\"\\nShape: \\n\", ex_spect.shape)\n",
    "print()\n",
    "\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.imshow(ex_spect, vmax=1)\n",
    "ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a673cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelSpectrogramDataset(Dataset):\n",
    "    def __init__(self, speech_files, labels, \n",
    "                 batch_size=16, max_length=150):\n",
    "        self.filenames = speech_files\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def getitem(self, idx):\n",
    "        spects, labels = [], []\n",
    "        start_idx = idx*self.batch_size\n",
    "        for filename, label in zip(self.filenames[start_idx:start_idx+batch_size],\n",
    "                                   self.labels[start_idx:start_idx+batch_size]):\n",
    "            spect, label = encode_sample(filename, label)\n",
    "            spects.append(torch.tensor(spect, dtype=torch.float32))\n",
    "                \n",
    "            labels.append(label)\n",
    "\n",
    "        spects = torch.cat(spects, dim=0)\n",
    "        labels = torch.tensor(labels)\n",
    "        return spects, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20487010",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "dataloader = MelSpectrogramDataset(speech_files, labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f23fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.getitem(0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d0bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.getitem(0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3346f4",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e0e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActDropNormCNN1D(nn.Module):\n",
    "    def __init__(self, n_feats, dropout, keep_shape=False):\n",
    "        super(ActDropNormCNN1D, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(n_feats)\n",
    "        self.keep_shape = keep_shape\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        # x = self.norm(self.dropout(F.gelu(x)))\n",
    "        x = self.dropout(F.gelu(self.norm(x)))\n",
    "        if self.keep_shape:\n",
    "            return x.transpose(1, 2)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class VoiceEntityClassification(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes, n_feats, num_layers, dropout):\n",
    "        super(VoiceEntityClassification, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(n_feats, n_feats, 10, 2, padding=10//2),\n",
    "            ActDropNormCNN1D(n_feats, dropout, keep_shape=True),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(n_feats, n_feats, 10, 2, padding=10//2),\n",
    "            ActDropNormCNN1D(n_feats, dropout),\n",
    "        )\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(n_feats*267, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        self.layer_norm2 = nn.LayerNorm(128)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.final_fc = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        n, hs = self.num_layers, self.hidden_size\n",
    "        return (torch.zeros(n*1, batch_size, hs).to(device),\n",
    "                torch.zeros(n*1, batch_size, hs).to(device))\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = x.to(device)\n",
    "        x = self.cnn1(x) # batch, channels, time, feature\n",
    "        x = self.cnn2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dense(x) # batch, time, feature\n",
    "        x = self.dropout2(F.gelu(self.layer_norm2(x)))  # (time, batch, n_class)\n",
    "        return self.final_fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2173a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 128\n",
    "\n",
    "speech_model = VoiceEntityClassification(hidden_size=d_model,\n",
    "                                         num_classes=len(characters),\n",
    "                                         n_feats=128,\n",
    "                                         num_layers=1,\n",
    "                                         dropout=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97dc354",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(summary(speech_model, torch.unsqueeze(dataloader.getitem(0)[0][0], 0), \n",
    "              speech_model._init_hidden(1), show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a9d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, optimizer, criterion, train=True):\n",
    "    global batch_size\n",
    "    total_loss = 0\n",
    "    for batch in range(len(dataloader) // batch_size):\n",
    "        spects, labels = dataloader.getitem(batch)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        hidden = model._init_hidden(batch_size)\n",
    "        logits = model(torch.tensor(spects), hidden)\n",
    "        loss = criterion(logits.to(torch.double), labels.to(torch.double).to(device))\n",
    "        \n",
    "        if train:\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "    \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d580e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af1edcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(object):\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = float(d_model)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "        self.iters = 0.0\n",
    "\n",
    "    def step(self):\n",
    "        self.iters += 1.0\n",
    "        arg1 = 1 / math.sqrt(self.iters)\n",
    "        arg2 = self.iters * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = 1 / math.sqrt(self.d_model) * min(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eab8f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_losses = []\n",
    "plot_val_losses = []\n",
    "\n",
    "def train(train_dataloader, val_dataloader, model, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    global plot_train_losses\n",
    "    global plot_val_losses\n",
    "    global d_model\n",
    "    print_train_loss_total = 0  # Reset every print_every\n",
    "    plot_train_loss_total = 0  # Reset every plot_every\n",
    "    \n",
    "    print_val_loss_total = 0  # Reset every print_every\n",
    "    plot_val_loss_total = 0\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                                optimizer, mode='min',\n",
    "                                factor=0.50, patience=6)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_epoch(train_dataloader, model, optimizer, criterion)\n",
    "        print_train_loss_total += train_loss\n",
    "        plot_train_loss_total += train_loss\n",
    "        \n",
    "        # Evaluate validation dataloader\n",
    "        val_loss = train_epoch(val_dataloader, model, optimizer, criterion, train=False)\n",
    "        print_val_loss_total += val_loss\n",
    "        plot_val_loss_total += val_loss\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_train_loss_avg = print_train_loss_total / print_every\n",
    "            print_train_loss_total = 0\n",
    "            print_val_loss_avg = print_val_loss_total / print_every\n",
    "            print_val_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_train_loss_avg, print_val_loss_avg\n",
    "                                             ))\n",
    "            print()\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_train_loss_avg = plot_train_loss_total / plot_every\n",
    "            plot_train_losses.append(plot_train_loss_avg)\n",
    "            plot_train_loss_total = 0\n",
    "            \n",
    "            plot_val_loss_avg = plot_val_loss_total / plot_every\n",
    "            plot_val_losses.append(plot_val_loss_avg)\n",
    "            plot_val_loss_total = 0\n",
    "\n",
    "    showPlot(plot_train_losses)\n",
    "    showPlot(plot_val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afed441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = MelSpectrogramDataset(speech_files[:4000], labels[:4000], \n",
    "                                   batch_size=batch_size)\n",
    "val_dataloader = MelSpectrogramDataset(speech_files[4000:], labels[4000:], \n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "train(train_dataloader, val_dataloader, speech_model, 15, \n",
    "      learning_rate=1e-3, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ffb4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(speech_model, \"my_voice.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e21e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1000\n",
    "file = my_speech_files[index]\n",
    "print(\"Speech file: \\n\", file)\n",
    "\n",
    "ex_wav, sr = load_wav(file)\n",
    "ex_spect = create_spect(ex_wav, sr)[0]\n",
    "print(\"Sample rate: \\n\", sr)\n",
    "print(\"\\nShape: \\n\", ex_spect.shape)\n",
    "print()\n",
    "\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.imshow(ex_spect, vmax=1)\n",
    "ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = speech_model(torch.unsqueeze(ex_spect, 0), speech_model._init_hidden(1))\n",
    "sigmoid = nn.Sigmoid()\n",
    "out = sigmoid(out)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a5f195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
