{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180c4f92",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f433c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models.decoder import ctc_decoder\n",
    "from torchaudio.utils import download_asset\n",
    "from pytorch_model_summary import summary\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio.functional as AF\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "from torchaudio.utils import download_asset\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "from playsound import playsound\n",
    "from natsort import natsorted\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import IPython\n",
    "import time\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d24029",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.switch_backend('agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcacb5c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef7d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_10M\n",
    "acoustic_model = bundle.get_model()\n",
    "\n",
    "print(\"Sample Rate:\", bundle.sample_rate)\n",
    "\n",
    "print(\"Labels:\", bundle.get_labels())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8797f",
   "metadata": {},
   "source": [
    "# Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54023188",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_files = os.listdir(\"/media/nathanmon/389E28739E282BB6/Users/Natha/Datasets/MySpeechData/my_voice\")\n",
    "speech_files = natsorted(speech_files)\n",
    "\n",
    "sentences = []\n",
    "with open(\"/media/nathanmon/389E28739E282BB6/Users/Natha/Datasets/MySpeechData/sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        sentences.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f4f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea99545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# while i < len(sentences):\n",
    "#     if len(sentences[i]) < 100:\n",
    "#         sentences.pop(i)\n",
    "#         speech_files.pop(i)\n",
    "#         i -= 1\n",
    "#     i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60dbf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3766fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The set of characters accepted in the transcription.\n",
    "characters = [x for x in \"abcdefghijklmnopqrstuvwxyz-|'\"]\n",
    "# Mapping characters to integers\n",
    "def char_to_num(sentence):\n",
    "    tokens = []\n",
    "    for char in sentence:\n",
    "        if char in characters:\n",
    "            tokens.append(characters.index(char))\n",
    "    return tokens\n",
    "\n",
    "def num_to_char(sentence):\n",
    "    chars = []\n",
    "    for char in sentence:\n",
    "        if char < len(characters):\n",
    "            chars.append(characters[char])\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e887160",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"/media/nathanmon/389E28739E282BB6/Users/Natha/Datasets/MySpeechData/my_voice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd806ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 8000\n",
    "max_length = 85000\n",
    "\n",
    "count = 0\n",
    "for i, file in enumerate(speech_files):\n",
    "    wav, sr = torchaudio.load(base + \"/\" + file)\n",
    "    wav = AF.resample(wav, sr, sample_rate)\n",
    "    if sr != 16000:\n",
    "        print(i)\n",
    "        print(file)\n",
    "    \n",
    "    if len(wav[0]) > 85000:\n",
    "        count += 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0df32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An integer scalar Tensor. The window length in samples.\n",
    "n_mels = 128\n",
    "# An integer scalar Tensor. The number of samples to step.\n",
    "win_length = 160\n",
    "# An integer scalar Tensor. The size of the FFT to apply.\n",
    "# If not provided, uses the smallest power of 2 enclosing frame_length.\n",
    "hop_length = 80\n",
    "\n",
    "def load_wav(filename):\n",
    "    wav, sr = torchaudio.load(base + \"/\" + filename)\n",
    "    if sample_rate != bundle.sample_rate:\n",
    "        wav = torchaudio.functional.resample(wav, sample_rate, bundle.sample_rate)\n",
    "    if len(wav[0]) < max_length:\n",
    "        wav = torch.concat((wav[0], torch.zeros(max_length - len(wav[0])))).unsqueeze(-2)\n",
    "    else:\n",
    "        cut_length = len(wav[0]) - max_length\n",
    "        wav = wav[0][cut_length//2:len(wav[0])-(cut_length//2)].unsqueeze(-2)\n",
    "        \n",
    "    mean = wav.mean()\n",
    "    std = wav.std()\n",
    "    wav = (wav - mean) / std\n",
    "        \n",
    "    return wav, sr\n",
    "\n",
    "def create_spect(wav, sr):\n",
    "    spect = torchaudio.transforms.MelSpectrogram(\n",
    "                                    sample_rate=sr, n_mels=n_mels,\n",
    "                                    win_length=win_length, \n",
    "                                    hop_length=hop_length\n",
    "    )(wav)\n",
    "    spect = np.log(spect + 1e-14)\n",
    "        \n",
    "    return spect\n",
    "\n",
    "def process_text(label):\n",
    "    label = label.lower()\n",
    "    label = label.replace(\" \", \"|\")\n",
    "    label = label.replace(\" -- \", \"|\")\n",
    "    label = label.replace(\"-\", \"|\")\n",
    "    label = label.replace(\";\", \"|\")\n",
    "    label = label.replace(\":\", \"|\")\n",
    "    label = char_to_num(label)\n",
    "    \n",
    "    return label\n",
    "\n",
    "def encode_sample(file, label):\n",
    "    wav, sr = load_wav(file)\n",
    "#     spect = create_spect(wav, sr)\n",
    "    label = process_text(label)\n",
    "        \n",
    "    return wav, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac271e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "file = speech_files[index]\n",
    "label = sentences[index]\n",
    "print(\"Speech file: \\n\", file)\n",
    "print(\"\\nSentence: \\n\", label)\n",
    "\n",
    "ex_wav, sr = load_wav(file)\n",
    "ex_spect = create_spect(ex_wav, sr)[0]\n",
    "print(\"Sample rate: \\n\", sr)\n",
    "print(\"\\nShape: \\n\", ex_spect.shape)\n",
    "print()\n",
    "\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.imshow(ex_spect, vmax=1)\n",
    "ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a10f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveformDataset(Dataset):\n",
    "    def __init__(self, speech_files, labels, \n",
    "                 batch_size=16, max_length=150):\n",
    "        self.filenames = speech_files\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def getitem(self, idx):\n",
    "        waves, labels = [], []\n",
    "        start_idx = idx*self.batch_size\n",
    "        for filename, label in zip(self.filenames[start_idx:start_idx+batch_size],\n",
    "                           self.labels[start_idx:start_idx+batch_size]):\n",
    "            wav, label = encode_sample(filename, label)\n",
    "            waves.append(torch.tensor(wav, dtype=torch.float32))\n",
    "            if len(label) < self.max_length:\n",
    "                zeros = [0] * (self.max_length - len(label))\n",
    "                label = label + zeros\n",
    "            elif len(label) > self.max_length:\n",
    "                label = label[:self.max_length]\n",
    "                \n",
    "            labels.append(label)\n",
    "\n",
    "        waves = torch.cat(waves, dim=0)\n",
    "        labels = torch.tensor(labels)\n",
    "        return waves, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f304d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "dataloader = WaveformDataset(speech_files, sentences, \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c50c5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.getitem(0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28808d3a",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41730ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEECH_FILE = download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\")\n",
    "\n",
    "speech_file = download_asset(\"tutorial-assets/ctc-decoding/1688-142285-0007.wav\")\n",
    "\n",
    "IPython.display.Audio(speech_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb28f876",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [label.lower() for label in bundle.get_labels()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e361decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_file = \"/media/nathanmon/389E28739E282BB6/Users/Natha/Datasets/MySpeechData/my_voice/0.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4803d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(speech_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c65b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(speech_file)\n",
    "\n",
    "if sample_rate != bundle.sample_rate:\n",
    "    waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc58be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_num(sentence):\n",
    "    nums = []\n",
    "    for char in sentence:\n",
    "        if char in tokens:\n",
    "            nums.append(tokens.index(char))\n",
    "    return nums\n",
    "\n",
    "def num_to_char(sentence):\n",
    "    chars = []\n",
    "    for char in sentence:\n",
    "        if char < len(tokens):\n",
    "            chars.append(tokens[char])\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab135e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CTCLoss(y_true, y_pred):\n",
    "    # Compute the training-time loss value\n",
    "    batch_len = y_true.shape[0]\n",
    "    input_length = y_pred.shape[0]\n",
    "    label_length = y_true.shape[1]\n",
    "\n",
    "    input_length = input_length * torch.ones(size=(batch_len, 1), dtype=torch.int64)\n",
    "    label_length = label_length * torch.ones(size=(batch_len, 1), dtype=torch.int64)\n",
    "\n",
    "    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "    \n",
    "    loss = criterion(y_pred, y_true, input_length, label_length)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f43a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "print(\"Num params: \", get_n_params(acoustic_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d36d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_transcript = \"i really was very much afraid of showing him how much shocked i was at some parts of what he said\"\n",
    "actual_transcript = \"Many non-infectious diseases have a partly or completely genetic basis and may thus be transmitted from one generation to another\"\n",
    "label = torch.tensor([char_to_num(actual_transcript)])\n",
    "actual_transcript = actual_transcript.split()\n",
    "\n",
    "out, hidden = acoustic_model(torch.tensor(waveform))\n",
    "out = F.log_softmax(out, dim=2)\n",
    "logits = out.transpose(0, 1)\n",
    "loss = CTCLoss(label, logits)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a2d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join(num_to_char(torch.unique_consecutive(torch.argmax(logits, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e7de0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acoustic_model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f7c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in acoustic_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2538f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_model.aux.weight.requires_grad = True\n",
    "acoustic_model.aux.bias.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f44b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in acoustic_model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fd825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyCTCDecoder(torch.nn.Module):\n",
    "    def __init__(self, labels, blank=0):\n",
    "        super().__init__()\n",
    "        self.labels = labels\n",
    "        self.blank = blank\n",
    "\n",
    "    def forward(self, emission: torch.Tensor) -> List[str]:\n",
    "        \"\"\"Given a sequence emission over labels, get the best path\n",
    "        Args:\n",
    "          emission (Tensor): Logit tensors. Shape `[num_seq, num_label]`.\n",
    "\n",
    "        Returns:\n",
    "          List[str]: The resulting transcript\n",
    "        \"\"\"\n",
    "        indices = torch.argmax(emission, dim=-1)  # [num_seq,]\n",
    "        indices = torch.unique_consecutive(indices, dim=-1)\n",
    "        indices = [i for i in indices if i != self.blank]\n",
    "        joined = \"\".join([self.labels[i] for i in indices])\n",
    "        return joined.replace(\"|\", \" \").strip().split()\n",
    "\n",
    "\n",
    "greedy_decoder = GreedyCTCDecoder(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c8d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_result = greedy_decoder(out[0])\n",
    "greedy_transcript = \" \".join(greedy_result)\n",
    "greedy_wer = torchaudio.functional.edit_distance(actual_transcript, greedy_result) / len(actual_transcript)\n",
    "\n",
    "print(f\"Transcript: {greedy_transcript}\")\n",
    "print(f\"WER: {greedy_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc9da56",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f48d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, optimizer, train=True):\n",
    "    global batch_size\n",
    "    total_loss = 0\n",
    "    for batch in range(len(dataloader) // batch_size):\n",
    "        waveform, labels = dataloader.getitem(batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits, hidden = model(torch.tensor(waveform)) # (B, N, C)\n",
    "        logits = F.log_softmax(logits, dim=2)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        logits = logits.transpose(0, 1) # (N, B, C)\n",
    "        loss = CTCLoss(labels, logits)\n",
    "        \n",
    "        if train:\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "    \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / (len(dataloader) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5451a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_losses = []\n",
    "plot_val_losses = []\n",
    "\n",
    "def train(train_dataloader, val_dataloader, model, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    global plot_train_losses\n",
    "    global plot_val_losses\n",
    "    global d_model\n",
    "    print_train_loss_total = 0  # Reset every print_every\n",
    "    plot_train_loss_total = 0  # Reset every plot_every\n",
    "    \n",
    "    print_val_loss_total = 0  # Reset every print_every\n",
    "    plot_val_loss_total = 0\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                                optimizer, mode='min',\n",
    "                                factor=0.50, patience=6)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_epoch(train_dataloader, model, optimizer)\n",
    "        print_train_loss_total += train_loss\n",
    "        plot_train_loss_total += train_loss\n",
    "        \n",
    "        # Evaluate validation dataloader\n",
    "        val_loss = train_epoch(val_dataloader, model, optimizer, train=False)\n",
    "        print_val_loss_total += val_loss\n",
    "        plot_val_loss_total += val_loss\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_train_loss_avg = print_train_loss_total / print_every\n",
    "            print_train_loss_total = 0\n",
    "            print_val_loss_avg = print_val_loss_total / print_every\n",
    "            print_val_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_train_loss_avg, print_val_loss_avg\n",
    "                                             ))\n",
    "            print()\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_train_loss_avg = plot_train_loss_total / plot_every\n",
    "            plot_train_losses.append(plot_train_loss_avg)\n",
    "            plot_train_loss_total = 0\n",
    "            \n",
    "            plot_val_loss_avg = plot_val_loss_total / plot_every\n",
    "            plot_val_losses.append(plot_val_loss_avg)\n",
    "            plot_val_loss_total = 0\n",
    "\n",
    "    showPlot(plot_train_losses)\n",
    "    showPlot(plot_val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adafff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CTCLoss(y_true, y_pred):\n",
    "    # Compute the training-time loss value\n",
    "    batch_len = y_true.shape[0]\n",
    "    input_length = y_pred.shape[0]\n",
    "    label_length = y_true.shape[1]\n",
    "    label_length = torch.tensor([len(seq) for seq in y_true], dtype=torch.int64).unsqueeze(1)\n",
    "\n",
    "    input_length = input_length * torch.ones(size=(batch_len, 1), dtype=torch.int64)\n",
    "    label_length = label_length * torch.ones(size=(batch_len, 1), dtype=torch.int64)\n",
    "\n",
    "    criterion = nn.CTCLoss(zero_infinity=True)\n",
    "    loss = criterion(y_pred, y_true, input_length, label_length)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b54f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d51162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "# acoustic_model = bundle.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acaa86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# 4000\n",
    "train_dataloader = WaveformDataset(speech_files[:4000], sentences[:4000], \n",
    "                                   batch_size=batch_size)\n",
    "val_dataloader = WaveformDataset(speech_files[4000:], sentences[4000:], \n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "train(train_dataloader, val_dataloader, acoustic_model, 5, \n",
    "      learning_rate=1e-5, print_every=1, plot_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efbb188",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in acoustic_model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de8bf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(acoustic_model.state_dict(), \"my_speech_recognition.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8342441c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
